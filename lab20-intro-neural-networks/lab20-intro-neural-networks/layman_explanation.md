# Layman Explanation of Lab 20

This lab was about understanding the **basics of neural networks** in plain terms.

- Think of a **neuron** as a tiny calculator that takes numbers, multiplies them by importance (weights), adds them, and then decides what to do with the result using a rule (activation function).

- We tested two rules:
  - **Sigmoid**: Squashes numbers into a range between 0 and 1, like squeezing any value into a small box.
  - **ReLU**: If the number is negative, it says 0; if positive, it just keeps the number.

- A **layer** is just a group of these neurons working together.  
  - First layer = Input (like raw data from sensors).  
  - Middle = Hidden (where the "thinking" happens).  
  - Last = Output (the answer or prediction).  

We even drew a picture showing a simple neural network with 3 inputs, 2 hidden neurons, and 1 output neuron.

In short: This lab introduced the *building blocks of AI brains*, showing how simple pieces combine to make powerful networks.
