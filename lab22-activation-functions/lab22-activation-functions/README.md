# Lab 22: Intro to Activation Functions

## Objectives
- Understand the role of activation functions in neural networks
- Learn about Sigmoid, Tanh, ReLU, and Leaky ReLU
- Implement various activation functions in an MLP
- Compare their effects on model performance

## Files
- task1_train_with_activations.py
- task1_leakyrelu.py
- task2_compare_results.py
- task3_summary.md
- troubleshooting.md
- interview_qna.md
- layman_explanation.md
- screenshot.png

## Instructions
Run the scripts step by step:
```bash
python3 task1_train_with_activations.py
python3 task1_leakyrelu.py
python3 task2_compare_results.py
```

