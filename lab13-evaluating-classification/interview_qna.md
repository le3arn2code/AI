# Lab 13 – Interview Q&A

1. **What is a confusion matrix?**
   - A table that summarizes prediction results of a classification model, showing correct and incorrect predictions.

2. **What does precision measure?**
   - The proportion of true positives among all predicted positives.

3. **What does recall measure?**
   - The proportion of true positives among all actual positives.

4. **When is F1-score useful?**
   - When you need a balance between precision and recall, especially with imbalanced datasets.

5. **What does high precision but low recall indicate?**
   - The model is conservative, predicts fewer positives but is usually correct.

6. **What does high recall but low precision indicate?**
   - The model captures most positives but with many false alarms.

7. **Why use a Random Forest Classifier in evaluation labs?**
   - It’s robust, easy to implement, and provides good baseline accuracy.

8. **What is overfitting in classification?**
   - When a model performs well on training data but poorly on unseen data.

9. **How do you handle imbalanced datasets in evaluation?**
   - Use techniques like oversampling, undersampling, or weighted metrics.

10. **What is the role of classification_report in scikit-learn?**
    - It provides precision, recall, F1-score, and support for each class.
